# Datasets for LLMs and Secret-Keeping Research

This directory contains datasets for investigating whether LLMs can keep secrets (e.g., plot twists) when conditioned on explicit plans.

## Dataset 1: WritingPrompts

### Overview
- **Source**: [HuggingFace - euclaise/writingprompts](https://huggingface.co/datasets/euclaise/writingprompts)
- **Original Source**: Reddit r/WritingPrompts subreddit
- **Size**: ~300K story-prompt pairs
- **Format**: HuggingFace Dataset (parquet)
- **Task**: Story generation from writing prompts
- **License**: Public domain (user-generated content)

### Description
The WritingPrompts dataset contains creative writing prompts from Reddit's r/WritingPrompts subreddit paired with user-written stories. This is the canonical dataset for evaluating hierarchical story generation, as used in the foundational paper "Hierarchical Neural Story Generation" (Fan et al., 2018).

### Why Relevant
- Contains diverse creative writing prompts that often involve plot twists and secrets
- Stories have natural narrative structure (exposition, conflict, climax, resolution)
- Enables evaluation of whether models "foreshadow" or "spoil" endings
- Large enough for statistical analysis

### Download Instructions

**Using HuggingFace (recommended):**
```python
from datasets import load_dataset

# Full dataset
dataset = load_dataset("euclaise/writingprompts")
dataset.save_to_disk("datasets/writingprompts")

# Or load with streaming for memory efficiency
dataset = load_dataset("euclaise/writingprompts", split="train", streaming=True)
```

**Alternative - curated version:**
```python
# Curated version with higher quality samples
dataset = load_dataset("euclaise/WritingPrompts_curated")
```

### Loading the Dataset
```python
from datasets import load_from_disk
dataset = load_from_disk("datasets/writingprompts")

# Access samples
for sample in dataset["train"]:
    prompt = sample["prompt"]
    story = sample["story"]
```

### Sample Data
See `writingprompts_samples.json` for 10 example prompt-story pairs.

---

## Dataset 2: ROCStories

### Overview
- **Source**: [Rochester NLP - ROCStories](https://cs.rochester.edu/nlp/rocstories/)
- **Size**: ~98K five-sentence stories + Story Cloze Test instances
- **Format**: CSV
- **Task**: Story completion, commonsense reasoning
- **License**: Research use (requires registration)

### Description
ROCStories is a corpus of five-sentence commonsense stories that capture causal and temporal relations between daily events. Each story has a clear beginning, middle, and end structure.

### Why Relevant
- Short, well-structured stories ideal for studying narrative coherence
- Story Cloze Test evaluates understanding of story endings
- Can be used to test if models reveal story endings prematurely
- Well-validated for commonsense story understanding

### Download Instructions

**Registration required:**
1. Visit https://cs.rochester.edu/nlp/rocstories/
2. Fill out the Google Form to request access
3. You will receive an email with download links

**After receiving access:**
```bash
# Download the files you receive
wget <url_from_email> -O datasets/rocstories/50KStories.csv
```

### Loading the Dataset
```python
import pandas as pd
stories = pd.read_csv("datasets/rocstories/50KStories.csv")
```

### Notes
- Requires registration - cannot be automatically downloaded
- Contains 5-sentence stories with clear narrative structure
- Includes Story Cloze Test for evaluating story understanding

---

## Dataset 3: Story Generation Evaluation (Semantic Scholar)

### Overview
- **Source**: [GitHub - lars76/story-evaluation-llm](https://github.com/lars76/story-evaluation-llm)
- **Size**: LLM-generated stories from 15+ models
- **Format**: JSON
- **Task**: Evaluating creative writing quality

### Description
A dataset of LLM-generated stories with quality evaluations across 15 models for training and benchmarking creative writing capabilities.

### Why Relevant
- Contains stories generated by various LLMs
- Pre-computed quality evaluations
- Can be used to compare foreshadowing across models

### Download Instructions
```bash
git clone https://github.com/lars76/story-evaluation-llm.git datasets/story-evaluation-llm
```

---

## Recommended Dataset for Experiments

For investigating whether LLMs reveal secrets when conditioned on plans:

### Primary: WritingPrompts
1. Contains natural "secrets" (plot twists in stories)
2. Large scale for statistical analysis
3. Diverse genres and narrative structures

### Experimental Design
1. **Filter stories with plot twists**: Use keywords or manual annotation to identify stories with clear plot twists/secrets
2. **Create explicit plans**: Generate outlines that include future plot twists
3. **Condition generation**: Test if models foreshadow the twist when given the plan vs. not
4. **Measure foreshadowing**: Use semantic similarity between generated text and "secret" content

### Preprocessing Notes
- Stories average 734 words in WritingPrompts
- Consider filtering by length (100-1000 words)
- Remove stories with formatting issues
- Consider genre-specific subsets (mystery, thriller work best for plot twists)
